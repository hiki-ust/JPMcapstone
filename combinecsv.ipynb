{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in a folder in time order while handling overlaps and duplicates.\n",
    "    \"\"\"\n",
    "    # Get all CSV files in the folder, sorted by modification time\n",
    "    csv_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith('.csv')],\n",
    "        key=lambda x: os.path.getctime(os.path.join(folder_path, x))\n",
    "    )\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the folder.\")\n",
    "        return\n",
    "\n",
    "    combined_data = None\n",
    "\n",
    "    for i, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Read the file\n",
    "        data = pd.read_csv(file_path, parse_dates=['time'])\n",
    "        data = data.iloc[:, :5]\n",
    "\n",
    "        # Check for overlap with the previous file\n",
    "        if combined_data is not None:\n",
    "            # Check if there's an overlap\n",
    "            overlap = combined_data['time'].iloc[-1] >= data['time'].iloc[0]\n",
    "            if not overlap:\n",
    "                raise ValueError(f\"Gap detected between files: {csv_files[i-1]} and {csv_files[i]}\")\n",
    "\n",
    "        # Append the data\n",
    "        combined_data = pd.concat([combined_data, data]) if combined_data is not None else data\n",
    "\n",
    "    # Drop duplicates based on the 'time' column and ensure all values in overlapping rows match\n",
    "    duplicates = combined_data.duplicated(subset=['time'], keep=False)\n",
    "    duplicate_rows = combined_data[duplicates]\n",
    "    if not duplicate_rows.empty:\n",
    "        mismatched_duplicates = duplicate_rows.groupby('time').nunique().max(axis=1)\n",
    "        if (mismatched_duplicates > 1).any():\n",
    "            #mismatched_df = duplicate_rows.loc[duplicate_rows['time'].isin(mismatched_duplicates[mismatched_duplicates > 1].index)]\n",
    "            #return mismatched_df \n",
    "            raise ValueError(f\"Mismatched duplicate rows found: {duplicate_rows}\")\n",
    "\n",
    "    combined_data = combined_data.drop_duplicates(subset=['time'], keep='first')\n",
    "\n",
    "    # Check if times are unique\n",
    "    if not combined_data['time'].is_unique:\n",
    "        raise ValueError(\"Non-unique timestamps found after removing duplicates.\")\n",
    "\n",
    "    # Save the combined file\n",
    "    stock_name = os.path.basename(folder_path)  # Use folder name as stock name\n",
    "    output_file = os.path.join(folder_path, f\"{stock_name}_combined.csv\")\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"Combined file saved to: {output_file}\")\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5.csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (1).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (2).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (3).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (4).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (5).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (6).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (7).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (8).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (9).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (10).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (11).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (12).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (13).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (14).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (15).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (16).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (17).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (18).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (19).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (20).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (21).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (22).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (23).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (24).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (25).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (26).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (27).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (28).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (29).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (30).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (31).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (32).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (33).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (34).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (35).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (36).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (37).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (38).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (39).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (40).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (41).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (42).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (43).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (44).csv\n",
      "Processing file: ./NSEBANK Front Month Future/NSE_BANKNIFTY1!, 5 (45).csv\n",
      "Combined file saved to: ./NSEBANK Front Month Future/_combined.csv\n"
     ]
    }
   ],
   "source": [
    "folder_paths = [\"./NSEBANK Front Month Future/\"]\n",
    "for folder_path in folder_paths:\n",
    "    combined_data = combine_csv_files(folder_path)\n",
    "    combined_data.to_csv(folder_path+'_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def check_trading_hours(data, trading_start='09:30:00', trading_end='16:00:00'):\n",
    "    \"\"\"\n",
    "    Check if all rows in the data fall within the trading hours.\n",
    "    \"\"\"\n",
    "    data['time_only'] = data['time'].dt.time\n",
    "    start_time = pd.to_datetime(trading_start).time()\n",
    "    end_time = pd.to_datetime(trading_end).time()\n",
    "\n",
    "    outside_hours = data[(data['time_only'] < start_time) | (data['time_only'] > end_time)]\n",
    "    if not outside_hours.empty:\n",
    "        print(\"Rows found outside trading hours:\")\n",
    "        print(outside_hours)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_trading_dates(data):\n",
    "    \"\"\"\n",
    "    Check if there are any missing dates in the data.\n",
    "    \"\"\"\n",
    "    data['date_only'] = data['time'].dt.date\n",
    "    all_dates = pd.date_range(\n",
    "        start=data['date_only'].min(),\n",
    "        end=data['date_only'].max(),\n",
    "        freq='B'  # Business days\n",
    "    )\n",
    "\n",
    "    missing_dates = set(all_dates.date) - set(data['date_only'])\n",
    "    if missing_dates:\n",
    "        print(\"Missing trading dates:\")\n",
    "        print(sorted(missing_dates))\n",
    "        return False\n",
    "    return True'\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
